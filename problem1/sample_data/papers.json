[
  {
    "arxiv_id": "9308101v1",
    "title": "Dynamic Backtracking",
    "authors": [
      "M. L. Ginsberg"
    ],
    "abstract": "Because of their occasional need to return to shallow points in a search tree, existing backtracking methods can sometimes erase meaningful progress toward solving a search problem. In this paper, we present a method by which backtrack points can be moved deeper in the search space, thereby avoiding this difficulty. The technique developed is a variant of dependency-directed backtracking that uses only polynomial space while still providing useful control information and retaining the completeness guarantees provided by earlier approaches.",
    "categories": [
      "cs.AI"
    ],
    "published": "1993-08-01T00:00:00Z",
    "updated": "1993-08-01T00:00:00Z",
    "abstract_stats": {
      "total_words": 80,
      "unique_words": 63,
      "total_sentences": 3,
      "avg_words_per_sentence": 26.666666666666668,
      "avg_word_length": 5.75
    }
  },
  {
    "arxiv_id": "9308102v1",
    "title": "A Market-Oriented Programming Environment and its Application to   Distributed Multicommodity Flow Problems",
    "authors": [
      "M. P. Wellman"
    ],
    "abstract": "Market price systems constitute a well-understood class of mechanisms that under certain conditions provide effective decentralization of decision making with minimal communication overhead. In a market-oriented programming approach to distributed problem solving, we derive the activities and resource allocations for a set of computational agents by computing the competitive equilibrium of an artificial economy. WALRAS provides basic constructs for defining computational market structures, and protocols for deriving their corresponding price equilibria. In a particular realization of this approach for a form of multicommodity flow problem, we see that careful construction of the decision process according to economic principles can lead to efficient distributed resource allocation, and that the behavior of the system can be meaningfully analyzed in economic terms.",
    "categories": [
      "cs.AI"
    ],
    "published": "1993-08-01T00:00:00Z",
    "updated": "1993-08-01T00:00:00Z",
    "abstract_stats": {
      "total_words": 121,
      "unique_words": 83,
      "total_sentences": 4,
      "avg_words_per_sentence": 30.25,
      "avg_word_length": 6.181818181818182
    }
  },
  {
    "arxiv_id": "9309101v1",
    "title": "An Empirical Analysis of Search in GSAT",
    "authors": [
      "I. P. Gent",
      "T. Walsh"
    ],
    "abstract": "We describe an extensive study of search in GSAT, an approximation procedure for propositional satisfiability. GSAT performs greedy hill-climbing on the number of satisfied clauses in a truth assignment. Our experiments provide a more complete picture of GSAT's search than previous accounts. We describe in detail the two phases of search: rapid hill-climbing followed by a long plateau search. We demonstrate that when applied to randomly generated 3SAT problems, there is a very simple scaling with problem size for both the mean number of satisfied clauses and the mean branching rate. Our results allow us to make detailed numerical conjectures about the length of the hill-climbing phase, the average gradient of this phase, and to conjecture that both the average score and average branching rate decay exponentially during plateau search. We end by showing how these results can be used to direct future theoretical analysis. This work provides a case study of how computer experiments can be used to improve understanding of the theoretical properties of algorithms.",
    "categories": [
      "cs.AI"
    ],
    "published": "1993-09-01T00:00:00Z",
    "updated": "1993-09-01T00:00:00Z",
    "abstract_stats": {
      "total_words": 170,
      "unique_words": 102,
      "total_sentences": 8,
      "avg_words_per_sentence": 21.25,
      "avg_word_length": 5.223529411764706
    }
  },
  {
    "arxiv_id": "9311101v1",
    "title": "The Difficulties of Learning Logic Programs with Cut",
    "authors": [
      "F. Bergadano",
      "D. Gunetti",
      "U. Trinchero"
    ],
    "abstract": "As real logic programmers normally use cut (!), an effective learning procedure for logic programs should be able to deal with it. Because the cut predicate has only a procedural meaning, clauses containing cut cannot be learned using an extensional evaluation method, as is done in most learning systems. On the other hand, searching a space of possible programs (instead of a space of independent clauses) is unfeasible. An alternative solution is to generate first a candidate base program which covers the positive examples, and then make it consistent by inserting cut where appropriate. The problem of learning programs with cut has not been investigated before and this seems to be a natural and reasonable approach. We generalize this scheme and investigate the difficulties that arise. Some of the major shortcomings are actually caused, in general, by the need for intensional evaluation. As a conclusion, the analysis of this paper suggests, on precise and technical grounds, that learning cut is difficult, and current induction techniques should probably be restricted to purely declarative logic languages.",
    "categories": [
      "cs.AI"
    ],
    "published": "1993-11-01T00:00:00Z",
    "updated": "1993-11-01T00:00:00Z",
    "abstract_stats": {
      "total_words": 173,
      "unique_words": 112,
      "total_sentences": 9,
      "avg_words_per_sentence": 19.22222222222222,
      "avg_word_length": 5.335260115606936
    }
  },
  {
    "arxiv_id": "9311102v1",
    "title": "Software Agents: Completing Patterns and Constructing User Interfaces",
    "authors": [
      "J. C. Schlimmer",
      "L. A. Hermens"
    ],
    "abstract": "To support the goal of allowing users to record and retrieve information, this paper describes an interactive note-taking system for pen-based computers with two distinctive features. First, it actively predicts what the user is going to write. Second, it automatically constructs a custom, button-box user interface on request. The system is an example of a learning-apprentice software- agent. A machine learning component characterizes the syntax and semantics of the user's information. A performance system uses this learned information to generate completion strings and construct a user interface. Description of Online Appendix: People like to record information. Doing this on paper is initially efficient, but lacks flexibility. Recording information on a computer is less efficient but more powerful. In our new note taking softwre, the user records information directly on a computer. Behind the interface, an agent acts for the user. To help, it provides defaults and constructs a custom user interface. The demonstration is a QuickTime movie of the note taking agent in action. The file is a binhexed self-extracting archive. Macintosh utilities for binhex are available from mac.archive.umich.edu. QuickTime is available from ftp.apple.com in the dts/mac/sys.soft/quicktime.",
    "categories": [
      "cs.AI"
    ],
    "published": "1993-11-01T00:00:00Z",
    "updated": "1993-11-01T00:00:00Z",
    "abstract_stats": {
      "total_words": 202,
      "unique_words": 115,
      "total_sentences": 22,
      "avg_words_per_sentence": 9.181818181818182,
      "avg_word_length": 5.257425742574258
    }
  },
  {
    "arxiv_id": "9312101v1",
    "title": "Decidable Reasoning in Terminological Knowledge Representation Systems",
    "authors": [
      "M. Buchheit",
      "F. M. Donini",
      "A. Schaerf"
    ],
    "abstract": "Terminological knowledge representation systems (TKRSs) are tools for designing and using knowledge bases that make use of terminological languages (or concept languages). We analyze from a theoretical point of view a TKRS whose capabilities go beyond the ones of presently available TKRSs. The new features studied, often required in practical applications, can be summarized in three main points. First, we consider a highly expressive terminological language, called ALCNR, including general complements of concepts, number restrictions and role conjunction. Second, we allow to express inclusion statements between general concepts, and terminological cycles as a particular case. Third, we prove the decidability of a number of desirable TKRS-deduction services (like satisfiability, subsumption and instance checking) through a sound, complete and terminating calculus for reasoning in ALCNR-knowledge bases. Our calculus extends the general technique of constraint systems. As a byproduct of the proof, we get also the result that inclusion statements in ALCNR can be simulated by terminological cycles, if descriptive semantics is adopted.",
    "categories": [
      "cs.AI"
    ],
    "published": "1993-12-01T00:00:00Z",
    "updated": "1993-12-01T00:00:00Z",
    "abstract_stats": {
      "total_words": 163,
      "unique_words": 108,
      "total_sentences": 8,
      "avg_words_per_sentence": 20.375,
      "avg_word_length": 5.877300613496932
    }
  },
  {
    "arxiv_id": "9401101v1",
    "title": "Teleo-Reactive Programs for Agent Control",
    "authors": [
      "N. Nilsson"
    ],
    "abstract": "A formalism is presented for computing and organizing actions for autonomous agents in dynamic environments. We introduce the notion of teleo-reactive (T-R) programs whose execution entails the construction of circuitry for the continuous computation of the parameters and conditions on which agent action is based. In addition to continuous feedback, T-R programs support parameter binding and recursion. A primary difference between T-R programs and many other circuit-based systems is that the circuitry of T-R programs is more compact; it is constructed at run time and thus does not have to anticipate all the contingencies that might arise over all possible runs. In addition, T-R programs are intuitive and easy to write and are written in a form that is compatible with automatic planning and learning methods. We briefly describe some experimental applications of T-R programs in the control of simulated and actual mobile robots.",
    "categories": [
      "cs.AI"
    ],
    "published": "1994-01-01T00:00:00Z",
    "updated": "1994-01-01T00:00:00Z",
    "abstract_stats": {
      "total_words": 152,
      "unique_words": 94,
      "total_sentences": 6,
      "avg_words_per_sentence": 25.333333333333332,
      "avg_word_length": 5.1118421052631575
    }
  },
  {
    "arxiv_id": "9402101v1",
    "title": "Learning the Past Tense of English Verbs: The Symbolic Pattern   Associator vs. Connectionist Models",
    "authors": [
      "C. X. Ling"
    ],
    "abstract": "Learning the past tense of English verbs - a seemingly minor aspect of language acquisition - has generated heated debates since 1986, and has become a landmark task for testing the adequacy of cognitive modeling. Several artificial neural networks (ANNs) have been implemented, and a challenge for better symbolic models has been posed. In this paper, we present a general-purpose Symbolic Pattern Associator (SPA) based upon the decision-tree learning algorithm ID3. We conduct extensive head-to-head comparisons on the generalization ability between ANN models and the SPA under different representations. We conclude that the SPA generalizes the past tense of unseen verbs better than ANN models by a wide margin, and we offer insights as to why this should be the case. We also discuss a new default strategy for decision-tree learning algorithms.",
    "categories": [
      "cs.AI"
    ],
    "published": "1994-02-01T00:00:00Z",
    "updated": "1994-02-01T00:00:00Z",
    "abstract_stats": {
      "total_words": 133,
      "unique_words": 89,
      "total_sentences": 6,
      "avg_words_per_sentence": 22.166666666666668,
      "avg_word_length": 5.2105263157894735
    }
  },
  {
    "arxiv_id": "9402102v1",
    "title": "Substructure Discovery Using Minimum Description Length and Background   Knowledge",
    "authors": [
      "D. J. Cook",
      "L. B. Holder"
    ],
    "abstract": "The ability to identify interesting and repetitive substructures is an essential component to discovering knowledge in structural data. We describe a new version of our SUBDUE substructure discovery system based on the minimum description length principle. The SUBDUE system discovers substructures that compress the original data and represent structural concepts in the data. By replacing previously-discovered substructures in the data, multiple passes of SUBDUE produce a hierarchical description of the structural regularities in the data. SUBDUE uses a computationally-bounded inexact graph match that identifies similar, but not identical, instances of a substructure and finds an approximate measure of closeness of two substructures when under computational constraints. In addition to the minimum description length principle, other background knowledge can be used by SUBDUE to guide the search towards more appropriate substructures. Experiments in a variety of domains demonstrate SUBDUE's ability to find substructures capable of compressing the original data and to discover structural concepts important to the domain. Description of Online Appendix: This is a compressed tar file containing the SUBDUE discovery system, written in C. The program accepts as input databases represented in graph form, and will output discovered substructures with their corresponding value.",
    "categories": [
      "cs.AI"
    ],
    "published": "1994-02-01T00:00:00Z",
    "updated": "1994-02-01T00:00:00Z",
    "abstract_stats": {
      "total_words": 197,
      "unique_words": 114,
      "total_sentences": 9,
      "avg_words_per_sentence": 21.88888888888889,
      "avg_word_length": 5.974619289340102
    }
  },
  {
    "arxiv_id": "9402103v1",
    "title": "Bias-Driven Revision of Logical Domain Theories",
    "authors": [
      "M. Koppel",
      "R. Feldman",
      "A. M. Segre"
    ],
    "abstract": "The theory revision problem is the problem of how best to go about revising a deficient domain theory using information contained in examples that expose inaccuracies. In this paper we present our approach to the theory revision problem for propositional domain theories. The approach described here, called PTR, uses probabilities associated with domain theory elements to numerically track the ``flow'' of proof through the theory. This allows us to measure the precise role of a clause or literal in allowing or preventing a (desired or undesired) derivation for a given example. This information is used to efficiently locate and repair flawed elements of the theory. PTR is proved to converge to a theory which correctly classifies all examples, and shown experimentally to be fast and accurate even for deep theories.",
    "categories": [
      "cs.AI"
    ],
    "published": "1994-02-01T00:00:00Z",
    "updated": "1994-02-01T00:00:00Z",
    "abstract_stats": {
      "total_words": 130,
      "unique_words": 80,
      "total_sentences": 6,
      "avg_words_per_sentence": 21.666666666666668,
      "avg_word_length": 5.223076923076923
    }
  }
]